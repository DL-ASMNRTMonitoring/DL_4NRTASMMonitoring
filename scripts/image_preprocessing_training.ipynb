{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2048a4a",
   "metadata": {},
   "source": [
    "# Load libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bacba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3.7\n",
    "\n",
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "from osgeo import gdal\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.python.keras import Model\n",
    "from tensorflow.python.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from segmentation_models.metrics import iou_score\n",
    "\n",
    "\n",
    "#Resizing images, if needed\n",
    "SIZE_X = 256 \n",
    "SIZE_Y = 256\n",
    "n_classes=3 #Number of classes for segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0ca1e",
   "metadata": {},
   "source": [
    "# Load datasets from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c721b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a fuction to load the various datasets \n",
    "def load_data(directory):\n",
    "    \"\"\"\n",
    "    Load multi-band .tif files from a directory into a NumPy array.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory containing .tif files.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Stack of .tif files as a 4D NumPy array (num_files, height, width, bands).\n",
    "        list: List of filenames in the order they were loaded.\n",
    "    \"\"\"\n",
    "    tif_files = [f for f in os.listdir(directory) if f.endswith('.tif')]\n",
    "    tif_files.sort()\n",
    "\n",
    "    arrays = []\n",
    "    filenames = []\n",
    "\n",
    "    for tif_file in tif_files:\n",
    "        file_path = os.path.join(directory, tif_file)\n",
    "        #print(f\"Loading {file_path}...\")\n",
    "\n",
    "        dataset = gdal.Open(file_path)\n",
    "        if dataset is None:\n",
    "            print(f\"Failed to load {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Get number of bands\n",
    "        num_bands = dataset.RasterCount\n",
    "        bands = []\n",
    "\n",
    "        # Read all bands\n",
    "        for i in range(1, num_bands + 1):\n",
    "            band = dataset.GetRasterBand(i)\n",
    "            bands.append(band.ReadAsArray())\n",
    "\n",
    "        # Stack bands along the last axis\n",
    "        array = np.stack(bands, axis=-1)\n",
    "        arrays.append(array)\n",
    "        filenames.append(tif_file)\n",
    "\n",
    "        dataset = None\n",
    "\n",
    "    if arrays:\n",
    "        stacked_array = np.stack(arrays, axis=0)\n",
    "        print(f\"Loaded {len(arrays)} .tif files into array of shape {stacked_array.shape}\")\n",
    "        return stacked_array, filenames\n",
    "    else:\n",
    "        print(\"No .tif files loaded.\")\n",
    "        return None, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks = load_data(\"path to training labels/\")\n",
    "y_train  = train_masks[0]\n",
    "\n",
    "train_images = load_data(\"path to train images/\")\n",
    "X_train = train_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb90e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train) # check the number of labels or uniques classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc1ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = load_data(\"path to validation images/\")\n",
    "X_val = val_images[0]\n",
    "\n",
    "val_masks = load_data(\"path to Validation labels/\")\n",
    "y_val = val_masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f391c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_images, val_masks, val_images, train_masks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb2f83",
   "metadata": {},
   "source": [
    "## data agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to rotate images and masks by a specific angle (90°, 180°, 270°)\n",
    "def rotate_image_stack(image_stack, mask, k):\n",
    "    \"\"\"\n",
    "    Rotate the image stack and mask by k * 90 degrees.\n",
    "    Args:\n",
    "        image_stack: 4D tensor of shape (time, height, width, channels).\n",
    "        mask: 3D tensor of shape (time, height, width).\n",
    "        k: Number of 90-degree rotations (1 for 90°, 2 for 180°, 3 for 270°).\n",
    "    Returns:\n",
    "        Rotated image stack and mask.\n",
    "    \"\"\"\n",
    "    image_stack = tf.image.rot90(image_stack, k=k)\n",
    "    mask = tf.image.rot90(mask, k=k)\n",
    "    return image_stack, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stack original and augmented datasets\n",
    "def stack_datasets(original_images, original_masks, *augmented_datasets):\n",
    "    \"\"\"\n",
    "    Stack the original dataset with augmented datasets.\n",
    "    Args:\n",
    "        original_images: Original 4D image stack.\n",
    "        original_masks: Original 3D mask.\n",
    "        *augmented_datasets: List of augmented (image_stack, mask) tuples.\n",
    "    Returns:\n",
    "        Stacked images and masks.\n",
    "    \"\"\"\n",
    "    # Combine all datasets into lists\n",
    "    all_images = [original_images] + [aug[0] for aug in augmented_datasets]\n",
    "    all_masks = [original_masks] + [aug[1] for aug in augmented_datasets]\n",
    "    \n",
    "    # Stack along the batch dimension (axis=0)\n",
    "    stacked_images = tf.concat(all_images, axis=0)\n",
    "    stacked_masks = tf.concat(all_masks, axis=0)\n",
    "    return stacked_images, stacked_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6949d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to flip images and masks in horizontally\n",
    "def random_flipHZ(image_stack, mask):\n",
    "    # Randomly flip horizontally\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image_stack = tf.image.flip_left_right(image_stack)\n",
    "        mask = tf.image.flip_left_right(mask)\n",
    "    \n",
    "    return image_stack, mask\n",
    "\n",
    "# Define a function to flip images and masks in vertically\n",
    "def random_flipVT(image_stack, mask):\n",
    "   \n",
    "    # Randomly flip vertically\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image_stack = tf.image.flip_up_down(image_stack)\n",
    "        mask = tf.image.flip_up_down(mask)\n",
    "    \n",
    "    return image_stack, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8261d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stack original and augmented datasets\n",
    "def stack_datasets2(original_images, original_masks, augmented_images, augmented_masks):\n",
    "    # Stack along the batch dimension (axis=0)\n",
    "    stacked_images = tf.concat([original_images, augmented_images], axis=0)\n",
    "    stacked_masks = tf.concat([original_masks, augmented_masks], axis=0)\n",
    "    return stacked_images, stacked_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5810d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow tensors\n",
    "image_stack = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "mask = tf.convert_to_tensor(y_train, dtype=tf.uint16)\n",
    "\n",
    "# Apply different rotations to create augmented datasets\n",
    "rotated_90, mask_90 = rotate_image_stack(image_stack, mask, k=1)  # 90°\n",
    "rotated_180, mask_180 = rotate_image_stack(image_stack, mask, k=2)  # 180°\n",
    "rotated_270, mask_270 = rotate_image_stack(image_stack, mask, k=3)  # 270°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d43740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the original dataset with all augmented datasets\n",
    "X_train, y_train = stack_datasets(\n",
    "    image_stack, mask,\n",
    "    (rotated_90, mask_90),\n",
    "    (rotated_180, mask_180),\n",
    "    (rotated_270, mask_270)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c1c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentation to create flipping augmented data\n",
    "augmented_imageVT, augmented_maskVT = random_flipVT(image_stack, mask) # vertical flip\n",
    "augmented_imageHZ, augmented_maskHZ = random_flipHZ(image_stack, mask) # horizontal flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d37316",
   "metadata": {},
   "outputs": [],
   "source": [
    "del rotated_90, mask_90, image_stack, mask,rotated_180, mask_180,rotated_270, mask_270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30bd249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the original and augmented datasets\n",
    "X_train, y_train = stack_datasets2(X_train, y_train, augmented_imageVT, augmented_maskVT)\n",
    "\n",
    "X_train, y_train = stack_datasets2(X_train, y_train, augmented_imageHZ, augmented_maskHZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511145f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del augmented_imageVT, augmented_maskVT, augmented_imageHZ, augmented_maskHZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2a51cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9bd8a",
   "metadata": {},
   "source": [
    "## Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c12e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Reshape to (num_samples * width * height, channels)\n",
    "samples, width, height, channels = X_train.shape\n",
    "X_train_reshaped = X_train.reshape(-1, channels)  # Shape: (156*256*256, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c635c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train_normalized = scaler.fit_transform(X_train_reshaped)  # Normalizes each channel\n",
    "\n",
    "# Save the standardize parameters to a file for model predictions\n",
    "#dump(scaler, 'scalerparameters_4model_predictions.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples, width2, height2, num_channels = X_val.shape\n",
    "X_val_reshaped = X_val.reshape(-1, num_channels)\n",
    "\n",
    "#load the saved standardise parameters and apply to validation dataset\n",
    "X_val_normalized = scaler.transform(X_val_reshaped)\n",
    "\n",
    "X_train = X_train_normalized.reshape(samples, width, height, channels)\n",
    "X_val = X_val_normalized.reshape(num_samples, width2, height2, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff8a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_normalized, X_val_normalized, X_val_reshaped, X_train_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd9da7b",
   "metadata": {},
   "source": [
    "## One hot encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d460eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=n_classes)\n",
    "y_val_cat = tf.keras.utils.to_categorical(y_val,num_classes=n_classes)\n",
    "\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835fa406",
   "metadata": {},
   "source": [
    "## weights calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights dynamically and halve the weight of the majority class\n",
    "def compute_weights(y_true):\n",
    "    # Flatten y_train to 1D array of class labels\n",
    "    y_flat = y_train.reshape(-1)\n",
    "\n",
    "    class_counts = np.bincount(y_flat, minlength=n_classes)\n",
    "\n",
    "    # Compute class weights\n",
    "    classes = np.unique(y_flat)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_flat)\n",
    "    \n",
    "    print(\"Class weights before adjustment:\", class_weights)\n",
    "\n",
    "    # Halve the weight of the majority class\n",
    "    majority_class_index = np.argmax(class_counts)\n",
    "    class_weights[majority_class_index] /= 2\n",
    "    print(\"Class weights After adjustment:\", class_weights)\n",
    "\n",
    "    return {i: class_weights[i] for i in classes}\n",
    "\n",
    "\n",
    "def normalize_class_weights(class_weights):\n",
    "    class_weights = np.array(class_weights, dtype=np.float32)\n",
    "    total = np.sum(class_weights)\n",
    "    if total == 0:\n",
    "        raise ValueError(\"Sum of class weights cannot be zero.\")\n",
    "    return class_weights / total\n",
    "\n",
    "\n",
    "# Compute class weights dynamically\n",
    "class_weights = compute_weights(y_train)\n",
    "\n",
    "class_weights = [class_weights[k] for k in sorted(class_weights.keys())]\n",
    "normalized_weights = normalize_class_weights(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5b03c7",
   "metadata": {},
   "source": [
    "## Apply loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba412082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice Loss function for multi-class segmentation\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = K.flatten(y_true)  # Flatten true labels\n",
    "    y_pred_f = K.flatten(y_pred)  # Flatten predictions\n",
    "    \n",
    "    intersection = K.sum(y_true_f * y_pred_f)  # Compute intersection\n",
    "    dice_coef = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)  # Dice coefficient\n",
    "    return 1 - dice_coef  # Dice loss\n",
    "\n",
    "'''\n",
    "A few useful metrics and losses\n",
    "'''\n",
    "def jacard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
    "\n",
    "def focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Focal Loss for multiclass segmentation.\n",
    "    \n",
    "    Args:\n",
    "        y_true: One-hot encoded ground truth.\n",
    "        y_pred: Predicted probabilities.\n",
    "        gamma: Focusing parameter.\n",
    "        alpha: Balancing factor.\n",
    "    \"\"\"\n",
    "    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1. - K.epsilon())\n",
    "    cross_entropy = -y_true * tf.math.log(y_pred)\n",
    "    focal = alpha * tf.pow(1 - y_pred, gamma) * cross_entropy\n",
    "    return tf.reduce_mean(tf.reduce_sum(focal, axis=-1))\n",
    "\n",
    "def combined_dice_focal_loss(gamma=2.0, alpha=0.25, weight_dice=0.5, weight_focal=0.5, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Combined Dice + Focal Loss.\n",
    "    \n",
    "    Args:\n",
    "        gamma: Focusing parameter for focal loss.\n",
    "        alpha: Balancing factor for focal loss.\n",
    "        weight_dice: Contribution of Dice Loss.\n",
    "        weight_focal: Contribution of Focal Loss.\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        dl = dice_loss(y_true, y_pred, smooth=smooth)\n",
    "        fl = focal_loss(y_true, y_pred, gamma=gamma, alpha=alpha)\n",
    "        return weight_dice * dl + weight_focal * fl\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cfdf4d",
   "metadata": {},
   "source": [
    "## Model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63239bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(IMG_WIDTH, IMG_HIGHT, IMG_CHANNELS):\n",
    "    inputs = tf.keras.layers.Input((IMG_WIDTH, IMG_HIGHT, IMG_CHANNELS))\n",
    "\n",
    "\n",
    "    #Contraction path\n",
    "    c1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "    c1 = tf.keras.layers.Dropout(0.25)(c1)\n",
    "    c1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = tf.keras.layers.Dropout(0.25)(c2)\n",
    "    c2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = tf.keras.layers.Dropout(0.25)(c3)\n",
    "    c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = tf.keras.layers.Dropout(0.25)(c4)\n",
    "    c4 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "    c5 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = tf.keras.layers.Dropout(0.25)(c5)\n",
    "    c5 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "    #Expansive path \n",
    "    u6 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = tf.keras.layers.concatenate([u6, c4])\n",
    "    c6 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "\n",
    "    u7 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "    c7 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "\n",
    "    u8 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "    c8 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "\n",
    "    u9 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "    c9 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_m, precision_m, recall_m])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "#Parameters for model\n",
    "\n",
    "IMG_HEIGHT = X_train.shape[1]\n",
    "IMG_WIDTH  = X_train.shape[2]\n",
    "IMG_CHANNELS = X_train.shape[3]\n",
    "num_labels = 3  #Multiclass\n",
    "input_shape = (IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS)\n",
    "\n",
    "model = unet_model(IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS)\n",
    "\n",
    "\n",
    "optimizer = Adam(lr=1e-3)\n",
    "    \n",
    "loss_fn = combined_dice_focal_loss(gamma=2.0, alpha=normalized_weights, weight_dice=0.6, weight_focal=0.4)\n",
    "    \n",
    "model.compile(optimizer=optimizer,loss= loss_fn,metrics=['accuracy', jacard_coef,iou_score])\n",
    "\n",
    "# Define learning rate reduction callback\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    factor=0.1,         # Factor by which to reduce (new_lr = lr * factor)\n",
    "    patience=5,         # Number of epochs with no improvement\n",
    "    verbose=1           # Print message when LR reduces\n",
    ")\n",
    "\n",
    "results = model.fit(X_train, y_train_cat, batch_size =16, verbose=1, epochs=30, validation_data=(X_val, y_val_cat), \n",
    "                    callbacks=[reduce_lr],\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Prediction over the validation dataset'''\n",
    "pred_test = model.predict(X_val)\n",
    "\n",
    "pred_test = np.argmax(pred_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c8b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model for predictions\n",
    "model.save('UNeTASMMonitoring_30_epochs_FocalLossAndDiceLoss.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346d5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
