{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7891fe4a",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f9c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "from osgeo import gdal\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump, load\n",
    "import matplotlib.colors as mcolors\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ed7e6",
   "metadata": {},
   "source": [
    "# Load Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ff26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a fuction to load the various datasets \n",
    "def load_data(directory):\n",
    "    \"\"\"\n",
    "    Load multi-band .tif files from a directory into a NumPy array.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory containing .tif files.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Stack of .tif files as a 4D NumPy array (num_files, height, width, bands).\n",
    "        list: List of filenames in the order they were loaded.\n",
    "    \"\"\"\n",
    "    tif_files = [f for f in os.listdir(directory) if f.endswith('.tif')]\n",
    "    tif_files.sort()\n",
    "\n",
    "    arrays = []\n",
    "    filenames = []\n",
    "\n",
    "    for tif_file in tif_files:\n",
    "        file_path = os.path.join(directory, tif_file)\n",
    "        #print(f\"Loading {file_path}...\")\n",
    "\n",
    "        dataset = gdal.Open(file_path)\n",
    "        if dataset is None:\n",
    "            print(f\"Failed to load {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Get number of bands\n",
    "        num_bands = dataset.RasterCount\n",
    "        bands = []\n",
    "        \n",
    "        # Read all bands\n",
    "        for i in range(1, num_bands + 1):\n",
    "            band = dataset.GetRasterBand(i)\n",
    "            bands.append(band.ReadAsArray())\n",
    "\n",
    "        # Stack bands along the last axis\n",
    "        array = np.stack(bands, axis=-1)\n",
    "        arrays.append(array)\n",
    "        filenames.append(tif_file)\n",
    "\n",
    "        dataset = None\n",
    "\n",
    "    if arrays:\n",
    "        stacked_array = np.stack(arrays, axis=0)\n",
    "        print(f\"Loaded {len(arrays)} .tif files into array of shape {stacked_array.shape}\")\n",
    "        return stacked_array, filenames\n",
    "    else:\n",
    "        print(\"No .tif files loaded.\")\n",
    "        return None, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resizing images, if needed\n",
    "SIZE_X = 256\n",
    "SIZE_Y = 256\n",
    "n_classes=3 #Number of classes for segmentation\n",
    "\n",
    "test_images = load_data(\"path to the test image/\")\n",
    "X_test = test_images[0]\n",
    "#X_test = X_test[:36,:,:,:2]\n",
    "test_masks = load_data(\"path to the test label/\") #if required\n",
    "y_test = test_masks[0]\n",
    "\n",
    "del test_masks, test_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea30c74f",
   "metadata": {},
   "source": [
    "# Standardisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffadbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples, width, height, num_channels = X_test.shape\n",
    "X_test_reshaped = X_test.reshape(-1, num_channels)\n",
    "\n",
    "#load the saved standardise parameters and apply to prediction dataset\n",
    "scaler = load(\"path to/pretrained_scaler_4model_predictions.joblib\")\n",
    "\n",
    "X_test_normalized = scaler.transform(X_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d00561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape to original size\n",
    "X_test = X_test_normalized.reshape(num_samples, width, height, num_channels)\n",
    "\n",
    "del X_test_reshaped, X_test_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6886e6f6",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc582677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the labels\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f870a0f5",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"path to pretrained saved model/UNeTASMMonitoring_30_epochs_FocalLossAndDiceLoss.hdf5\"\n",
    "\n",
    "#Load one model at a time for testing.\n",
    "Attmodel = tf.keras.models.load_model(path, compile=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce33509",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(width, height, num_channels)\n",
    "\n",
    "def predict_large_image(image, model, patch_size=256, num_classes=3, overlap=0.2):\n",
    "    \"\"\"\n",
    "    Predict a large image using a UNet model trained on smaller patches.\n",
    "    \n",
    "    Parameters:\n",
    "        image (np.array): (H, W, C) input image (standardized/normalized as required by model)\n",
    "        model (tf.keras.Model): Trained UNet model\n",
    "        patch_size (int): Size of patches used during training\n",
    "        num_classes (int): Number of output classes (for softmax)\n",
    "        overlap (float): Fraction of overlap between patches, e.g., 0.2 = 20%\n",
    "    \n",
    "    Returns:\n",
    "        output (np.array): Predicted mask (H, W, num_classes)\n",
    "    \"\"\"\n",
    "    H, W, C = image.shape\n",
    "    stride = int(patch_size * (1 - overlap))\n",
    "    output = np.zeros((H, W, num_classes))\n",
    "    count = np.zeros((H, W, num_classes))  # For overlapping average\n",
    "\n",
    "    for y in range(0, H - patch_size + 1, stride):\n",
    "        for x in range(0, W - patch_size + 1, stride):\n",
    "            patch = image[y:y+patch_size, x:x+patch_size, :]\n",
    "            patch = np.expand_dims(patch, axis=0)  # (1, patch_size, patch_size, C)\n",
    "            pred = model.predict(patch, verbose=0)[0]  # (patch_size, patch_size, num_classes)\n",
    "            \n",
    "            output[y:y+patch_size, x:x+patch_size, :] += pred\n",
    "            count[y:y+patch_size, x:x+patch_size, :] += 1\n",
    "\n",
    "    # Handle border patches (if needed)\n",
    "    for y in [H - patch_size]:\n",
    "        for x in range(0, W - patch_size + 1, stride):\n",
    "            patch = image[y:y+patch_size, x:x+patch_size, :]\n",
    "            patch = np.expand_dims(patch, axis=0)\n",
    "            pred = model.predict(patch, verbose=0)[0]\n",
    "            output[y:y+patch_size, x:x+patch_size, :] += pred\n",
    "            count[y:y+patch_size, x:x+patch_size, :] += 1\n",
    "\n",
    "    for y in range(0, H - patch_size + 1, stride):\n",
    "        for x in [W - patch_size]:\n",
    "            patch = image[y:y+patch_size, x:x+patch_size, :]\n",
    "            patch = np.expand_dims(patch, axis=0)\n",
    "            pred = model.predict(patch, verbose=0)[0]\n",
    "            output[y:y+patch_size, x:x+patch_size, :] += pred\n",
    "            count[y:y+patch_size, x:x+patch_size, :] += 1\n",
    "\n",
    "    # Bottom-right corner\n",
    "    patch = image[H-patch_size:H, W-patch_size:W, :]\n",
    "    patch = np.expand_dims(patch, axis=0)\n",
    "    pred = model.predict(patch, verbose=0)[0]\n",
    "    output[H-patch_size:H, W-patch_size:W, :] += pred\n",
    "    count[H-patch_size:H, W-patch_size:W, :] += 1\n",
    "\n",
    "    # Normalize by count to get averaged prediction\n",
    "    output = output / np.maximum(count, 1e-7)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c583ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Prediction over the test dataset'''\n",
    "pred_test = predict_large_image(X_test, Attmodel, patch_size=256, num_classes=3, overlap=0.5)\n",
    "\n",
    "\n",
    "pred_test = np.argmax(pred_test, axis=-1)\n",
    "print(pred_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a943c",
   "metadata": {},
   "source": [
    "# Visualise Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e62036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Define hex color mapping for each class\n",
    "class_info = {\n",
    "    0: (\"#023020\", \"Vegetation\"),  # Dark green\n",
    "    1: (\"#ffbf00\", \"Mines\"),  # Gold\n",
    "    2: (\"#999999\", \"Other\"),  # Med Gray\n",
    "}\n",
    "\n",
    "\n",
    "# Separate colors and names\n",
    "color_list = [class_info[i][0] for i in range(len(class_info))]\n",
    "class_names = [class_info[i][1] for i in range(len(class_info))]\n",
    "\n",
    "# Create colormap and norm\n",
    "cmap = mcolors.ListedColormap(color_list)\n",
    "bounds = np.arange(len(class_info) + 1)\n",
    "norm = mcolors.BoundaryNorm(boundaries=bounds, ncolors=len(class_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = random.randint(0, len(pred_test))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(pred_test,cmap=cmap, interpolation='nearest')\n",
    "plt.title('Predicted')\n",
    "plt.axis('off')\n",
    "#plt.savefig(\"West.png\", dpi=600, bbox_inches='tight', pad_inches=0.0)\n",
    "plt.subplot(122)\n",
    "plt.imshow(y_test[0, :, :, 0],cmap=cmap, interpolation='nearest')\n",
    "plt.title('True Label')\n",
    "plt.axis('off')\n",
    "#plt.colorbar(ticks=range(len(class_colors)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd86d1",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b83dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_pred = predict_large_image(X_test, Attmodel, patch_size=256, num_classes=3, overlap=0.5)\n",
    "CNN_pred_argmax = np.argmax(CNN_pred, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2007e128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, jaccard_score\n",
    "\n",
    "def evaluate_segmentation(y_true, y_pred, class_names=None):\n",
    "    \"\"\"\n",
    "    Compute per-class and overall Precision, Recall, IoU, F1-score, and Accuracy \n",
    "    for multi-class semantic segmentation, rounded to 4 decimal places.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (numpy array): Ground truth segmentation masks (one-hot encoded) - shape (N, H, W, C)\n",
    "        y_pred (numpy array): Predicted segmentation masks (probabilities) - shape (N, H, W, C)\n",
    "        class_names (list): Optional list of class names. Length must equal number of classes (C).\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing per-class and overall metrics.\n",
    "    \"\"\"\n",
    "    # Convert to class indices\n",
    "    y_true_labels = np.argmax(y_true, axis=-1).flatten()\n",
    "    y_pred_labels = np.argmax(y_pred, axis=-1).flatten()\n",
    "    \n",
    "    num_classes = y_true.shape[-1]\n",
    "    class_indices = list(range(num_classes))\n",
    "\n",
    "    # Default class names if not provided\n",
    "    if class_names is None:\n",
    "        class_names = [f\"Class_{i}\" for i in class_indices]\n",
    "\n",
    "    # --- Per-class metrics ---\n",
    "    precision_per_class = precision_score(y_true_labels, y_pred_labels, average=None, labels=class_indices, zero_division=1)\n",
    "    recall_per_class = recall_score(y_true_labels, y_pred_labels, average=None, labels=class_indices, zero_division=1)\n",
    "    f1_per_class = f1_score(y_true_labels, y_pred_labels, average=None, labels=class_indices, zero_division=1)\n",
    "    iou_per_class = jaccard_score(y_true_labels, y_pred_labels, average=None, labels=class_indices)\n",
    "\n",
    "    # --- Overall metrics (macro) ---\n",
    "    precision_macro = precision_score(y_true_labels, y_pred_labels, average='macro', zero_division=1)\n",
    "    recall_macro = recall_score(y_true_labels, y_pred_labels, average='macro', zero_division=1)\n",
    "    f1_macro = f1_score(y_true_labels, y_pred_labels, average='macro', zero_division=1)\n",
    "    iou_macro = jaccard_score(y_true_labels, y_pred_labels, average='macro')\n",
    "    accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
    "\n",
    "    # --- Organize per-class metrics into readable dict ---\n",
    "    per_class_metrics = {\n",
    "        name: {\n",
    "            \"Precision\": round(float(p), 2),\n",
    "            \"Recall\": round(float(r), 2),\n",
    "            \"F1-score\": round(float(f), 2),\n",
    "            \"IoU\": round(float(i), 2)\n",
    "        }\n",
    "        for name, p, r, f, i in zip(class_names, precision_per_class, recall_per_class, f1_per_class, iou_per_class)\n",
    "    }\n",
    "\n",
    "    # --- Final dictionary ---\n",
    "    results = {\n",
    "        \"Per-Class Metrics\": per_class_metrics,\n",
    "        \"Precision\": round(precision_macro, 2),\n",
    "        \"Recall\": round(recall_macro, 2),\n",
    "        \"F1\": round(f1_macro, 2),\n",
    "        \"Mean IoU\": round(iou_macro, 2),\n",
    "        \"Accuracy\": round(accuracy, 2)\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d7d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Vegetation\", \"Mines\", \"Other\"]\n",
    "\n",
    "metrics = evaluate_segmentation(y_test_cat, CNN_pred, class_names)\n",
    "for cls, vals in metrics[\"Per-Class Metrics\"].items():\n",
    "    print(f\"{cls}: {vals}\")\n",
    "\n",
    "print(\"\\nOverall Metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    if k != \"Per-Class Metrics\":\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
